image:
  # image registry to pull Mayastor images
  registry: docker.io
  # image registry's namespace where all Mayastor images exist (default: mayadata)
  repo: mayadata
  # release tag for OpenEBS Mayastor images
  tag: v2.0.0
  # imagePullPolicy for all Mayastor images
  pullPolicy: Always

# Node labels for pod assignment
# ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
# Note that if multi-arch images support 'kubernetes.io/arch: amd64'
# should be removed and set 'nodeSelector' to empty '{}' as default value.
nodeSelector:
  kubernetes.io/arch: amd64

base:
  # request timeout for rest & core agents
  default_req_timeout: 5s
  # cache timeout for core agent & diskpool deployment
  cache_poll_period: 30s
  # silence specific module components
  logSilenceLevel:
  initContainers:
    enabled: true
    containers:
      - name: agent-core-grpc-probe
        image: busybox:latest
        command: [ 'sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-agent-core 50051; do date; echo "Waiting for agent-core-grpc services..."; sleep 1; done;' ]
      - name: etcd-probe
        image: busybox:latest
        command: [ 'sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-etcd {{ .Values.etcd.service.port }}; do date; echo "Waiting for etcd..."; sleep 1; done;' ]

  initCoreContainers:
    enabled: true
    containers:
      - name: etcd-probe
        image: busybox:latest
        command: [ 'sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-etcd {{ .Values.etcd.service.port }}; do date; echo "Waiting for etcd..."; sleep 1; done;' ]
  # docker-secrets required to pull images if the container registry from image.Registry is protected
  imagePullSecrets:
    # enable imagePullSecrets for pulling Mayastor container images
    enabled: false
    # name of the imagePullSecret in the mayastor-namespace
    secrets:
      - name: login

  metrics:
    # enable metrics exporter
    enabled: true
    # metrics refresh time
    # WARNING: Lowering pollingInterval value will affect performance adversely
    pollingInterval: "5m"

  jaeger:
    # enable jaeger tracing
    enabled: false
    initContainer: true
    agent:
      name: jaeger-agent
      port: 6831
      initContainer:
        - name: jaeger-probe
          image: busybox:latest
          command: [ 'sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 -u {{.Values.base.jaeger.agent.name}} {{.Values.base.jaeger.agent.port}}; do date; echo "Waiting for jaeger..."; sleep 1; done;' ]

  initRestContainer:
    enabled: true
    initContainer:
      - name: api-rest-probe
        image: busybox:latest
        command: ['sh', '-c', 'trap "exit 1" TERM; until nc -vzw 5 {{ .Release.Name }}-api-rest 8081; do date; echo "Waiting for REST API endpoint to become available"; sleep 1; done;']

operators:
  pool:
    # logLevel for diskpool operator & other dependent crate
    logLevel: info
    resources:
      limits:
        # cpu limits for diskpool operator
        cpu: "100m"
        # memory limits for diskpool operator
        memory: "32Mi"
      requests:
        # cpu requests for diskpool operator
        cpu: "50m"
        # memory requests for diskpool operator
        memory: "16Mi"

jaeger-operator:
  # name of jaeger operator
  name: "{{ .Release.Name }}"
  crd:
    # install jaeger CRDs
    install: false
  jaeger:
    # install jaeger-operator
    create: false
  rbac:
    # create a clusterRole for Jaeger
    clusterRole: true

agents:
  core:
    # logLevel for the core crate
    logLevel: info
    resources:
      limits:
        # cpu limits for core agents
        cpu: "1000m"
        # memory limits for core agents
        memory: "32Mi"
      requests:
        # cpu requests for core agents
        cpu: "500m"
        # memory requests for core agents
        memory: "16Mi"
  ha:
    enabled: false
    node:
      logLevel: info
      resources:
        limits:
          # cpu limits for ha node agent
          cpu: "100m"
          # memory limits for ha node agent
          memory: "64Mi"
        requests:
          # cpu requests for ha node agent
          cpu: "100m"
          # memory requests for ha node agent
          memory: "64Mi"

apis:
  rest:
    # logLevel for the rest crate
    logLevel: info
    resources:
      limits:
        # cpu limits for rest
        cpu: "100m"
        # memory limits for rest
        memory: "64Mi"
      requests:
        # cpu requests for rest
        cpu: "50m"
        # memory requests for rest
        memory: "32Mi"

csi:
  image:
    # image registry to pull all CSI Sidecar images
    registry: k8s.gcr.io
    # image registry's namespace
    repo: sig-storage
    # imagePullPolicy for all CSI Sidecar images
    pullPolicy: IfNotPresent
    # csi-provisioner image release tag
    provisionerTag: v2.2.1
    # csi-attacher image release tag
    attacherTag: v3.2.1
    # csi-node-driver-registrar image release tag
    registrarTag: v2.1.0

  controller:
  # logLevel for the csi controller
    logLevel: info
    resources:
      limits:
        # cpu limits for csi controller
        cpu: "32m"
        # memory limits for csi controller
        memory: "128Mi"
      requests:
        # cpu requests for csi controller
        cpu: "16m"
        # memory requests for csi controller
        memory: "64Mi"
  node:
    logLevel: info
    topology:
      segments:
        openebs.io/csi-node: mayastor
      # add topology segments to the csi-node daemonset node selector
      nodeSelector: false
    resources:
      limits:
        # cpu limits for csi node plugin
        cpu: "100m"
        # memory limits for csi node plugin
        memory: "128Mi"
      requests:
        # cpu requests for csi node plugin
        cpu: "100m"
        # memory requests for csi node plugin
        memory: "64Mi"
    nvme:
      # nvme_core module io timeout in seconds
      io_timeout: "30"
      # ctrl_loss_tmo (controller loss timeout) in seconds
      ctrl_loss_tmo: "1980"
    kubeletDir: /var/lib/kubelet

mayastor:
  # logLevel for the core crate and other dependent crates
  logLevel: info,io_engine=info
  target:
    nvmf:
      # NVMF target interface (ip, mac, name or subnet)
      iface: ""
  cpuCount: "2"
  # node selectors to designate mayastor nodes for diskpool creation
  # Note that if multi-arch images support 'kubernetes.io/arch: amd64'
  # should be removed.
  nodeSelector:
    openebs.io/engine: mayastor
    kubernetes.io/arch: amd64
  resources:
    limits:
      # cpu limits for mayastor component
      cpu: ""
      # memory limits for mayastor component
      memory: "1Gi"
      # hugepage size available on the nodes
      hugepages2Mi: "2Gi"
    requests:
      # cpu requests for mayastor component
      cpu: ""
      # memory requests for csi node plugin
      memory: "1Gi"
      # hugepage size available on the nodes
      hugepages2Mi: "2Gi"

etcd:
  # etcd pod labels; okay to remove the openebs logging label if required
  podLabels:
    app: etcd
    openebs.io/logging: "true"
  # number of replicas of etcd
  replicaCount: 3
  # kubernetes Cluster Domain
  clusterDomain: cluster.local
  # TLS authentication for client-to-server communications
  # ref: https://etcd.io/docs/current/op-guide/security/
  client:
    secureTransport: false
  # TLS authentication for server-to-server communications
  # ref: https://etcd.io/docs/current/op-guide/security/
  peer:
    secureTransport: false
  # Enable persistence using Persistent Volume Claims
  persistence:
    # If true, use a Persistent Volume Claim. If false, use emptyDir.
    enabled: true
    # storageClass will define which storageClass to use in etcd's StatefulSets
    # a `manual` storageClass will provision a hostpath PV on the same node
    # an empty storageClass will use the default StorageClass on the cluster
    storageClass: ""
    # etcd volume size
    size: 2Gi
    # etcd's PVC's reclaimPolicy
    reclaimPolicy: "Delete"
  # Use a PreStop hook to remove the etcd members from the etcd cluster on container termination
  # Ignored if lifecycleHooks is set or replicaCount=1
  removeMemberOnContainerTermination: false

  auth:
    rbac:
      enabled: false
      allowNoneAuthentication: true
  # Init containers parameters:
  # volumePermissions: Change the owner and group of the persistent volume mountpoint to runAsUser:fsGroup values from the securityContext section.
  #
  volumePermissions:
    # chown the mounted volume; this is required if a statically provisioned hostpath volume is used
    enabled: true
  # extra debug information on logs
  debug: false
  initialClusterState: "new"
  # Pod anti-affinity preset
  # Ref: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity
  podAntiAffinityPreset: "hard"

  # etcd service parameters defines how the etcd service is exposed
  service:
    # K8s service type
    type: ClusterIP

    # etcd client port
    port: 2379

    # Specify the nodePort(s) value(s) for the LoadBalancer and NodePort service types.
    # ref: https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport
    #
    nodePorts:
      # Port from where etcd endpoints are accessible from outside cluster
      clientPort: 31379
      peerPort: ""

loki-stack:
  # enable loki log collection for Mayastor components
  enabled: true
  loki:
    rbac:
      # create rbac roles for loki
      create: true
      pspEnabled: false
    # enable loki installation as part of loki-stack
    enabled: true
    # install loki with persistence storage
    persistence:
      # enable persistence storage for the logs
      enabled: true
      # storageClass for Loki's centralised log storage
      # empty storageClass implies cluster default storageClass & `manual` creates a static hostpath PV
      storageClassName: ""
      # loki pvc's ReclaimPolicy, can be Delete or Retain
      reclaimPolicy: "Delete"
      # size of Loki's persistence storage
      size: 10Gi
    # loki process run & file permissions, required if sc=manual
    securityContext:
      fsGroup: 1001
      runAsGroup: 1001
      runAsNonRoot: false
      runAsUser: 1001
    # initContainers to chown the static hostpath PV by 1001 user
    initContainers:
      - command: ["/bin/bash", "-ec", "chown -R 1001:1001 /data"]
        image: docker.io/bitnami/bitnami-shell:10
        imagePullPolicy: IfNotPresent
        name: volume-permissions
        securityContext:
          runAsUser: 0
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
        volumeMounts:
        - mountPath: /data
          name: storage
    config:
      # Compactor is a MayastorDB(loki term) Shipper specific service that reduces the index
      # size by deduping the index and merging all the files to a single file per table.
      # Ref: https://grafana.com/docs/loki/latest/operations/storage/retention/
      compactor:
        # Dictates how often compaction and/or retention is applied. If the
        # Compactor falls behind, compaction and/or retention occur as soon as possible.
        compaction_interval: 20m

        # If not enabled compactor will only compact table but they will not get
        # deleted
        retention_enabled: true

        # The delay after which the compactor will delete marked chunks
        retention_delete_delay: 1h

        # Specifies the maximum quantity of goroutine workers instantiated to
        # delete chunks
        retention_delete_worker_count: 50

      # Rentention period of logs is configured within the limits_config section
      limits_config:
        # configuring retention period for logs
        retention_period: 168h

    # Loki service parameters defines how the Loki service is exposed
    service:
      # K8s service type
      type: ClusterIP
      port: 3100
      # Port where REST endpoints of Loki are accessible from outside cluster
      nodePort: 31001

  # promtail configuration
  promtail:
    rbac:
      # create rbac roles for promtail
      create: true
      pspEnabled: false
    # enabled enables promtail for scraping logs from nodes
    enabled: true
    # Disallow promtail from running on the master node
    tolerations: []
    config:
      # The Loki address to post logs to
      lokiAddress: http://{{ .Release.Name }}-loki:3100/loki/api/v1/push
      snippets:
          # Promtail will export logs to loki only based on based on below
          # configuration, below scrape config will export only mayastor services
          # which are labeled with openebs.io/logging=true
        scrapeConfigs: |
          - job_name: mayastor-pods-name
            pipeline_stages:
              - docker: {}
            kubernetes_sd_configs:
            - role: pod
            relabel_configs:
            - source_labels:
              - __meta_kubernetes_pod_node_name
              target_label: hostname
              action: replace
            - action: labelmap
              regex: __meta_kubernetes_pod_label_(.+)
            - action: keep
              source_labels:
              - __meta_kubernetes_pod_label_openebs_io_logging
              regex: true
              target_label: mayastor_component
            - action: replace
              replacement: $1
              separator: /
              source_labels:
              - __meta_kubernetes_namespace
              target_label: job
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_name
              target_label: pod
            - action: replace
              source_labels:
              - __meta_kubernetes_pod_container_name
              target_label: container
            - replacement: /var/log/pods/*$1/*.log
              separator: /
              source_labels:
              - __meta_kubernetes_pod_uid
              - __meta_kubernetes_pod_container_name
              target_label: __path__
obs:
  callhome:
    # Enable callhome
    enabled: true
    # logLevel for callhome
    logLevel: "info"
    resources:
      limits:
        # cpu limits for callhome
        cpu: "100m"
        # memory limits for callhome
        memory: "32Mi"
      requests:
        # cpu requests for callhome
        cpu: "50m"
        # memory requests for callhome
        memory: "16Mi"
